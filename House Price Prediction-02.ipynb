{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9291fdad",
   "metadata": {},
   "source": [
    "# Task 02 — Linear Regression: House Price Prediction (Python)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f1e5979",
   "metadata": {},
   "source": [
    "\n",
    "Description:\n",
    "\n",
    "In this project, you will predict the price of a house using various features like example number of rooms, size of the house, location and age of the house etc.\n",
    "\n",
    "The goal is to use Linear Regression in Python to build a model that learns the relationship between these features and house prices.\n",
    "\n",
    "Learning skills: Learning of regression analysis, Python, Scikit learn library.\n",
    "\n",
    "Goal. Predict house prices from numeric and categorical features using a scikit-learn pipeline.\n",
    "\n",
    "Load the dataset, Understand the data (EDA), Handle missing values, Explore relationships between features and SalePrice, Build a simple prediction model, Improve the model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2c5e30b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/nafkem/DS/venv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "sns.set (style=\"whitegrid\")\n",
    "\n",
    "import os\n",
    "print(os.getcwd())\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6109213",
   "metadata": {},
   "source": [
    "1. Load the dataset\n",
    "Load the train.csv file, which contains both the features (house details) and the target variable (SalePrice)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1c8126f7",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'train.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mFileNotFoundError\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m df = \u001b[43mpd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mtrain.csv\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/DS/venv/lib/python3.12/site-packages/pandas/io/parsers/readers.py:1026\u001b[39m, in \u001b[36mread_csv\u001b[39m\u001b[34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[39m\n\u001b[32m   1013\u001b[39m kwds_defaults = _refine_defaults_read(\n\u001b[32m   1014\u001b[39m     dialect,\n\u001b[32m   1015\u001b[39m     delimiter,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1022\u001b[39m     dtype_backend=dtype_backend,\n\u001b[32m   1023\u001b[39m )\n\u001b[32m   1024\u001b[39m kwds.update(kwds_defaults)\n\u001b[32m-> \u001b[39m\u001b[32m1026\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/DS/venv/lib/python3.12/site-packages/pandas/io/parsers/readers.py:620\u001b[39m, in \u001b[36m_read\u001b[39m\u001b[34m(filepath_or_buffer, kwds)\u001b[39m\n\u001b[32m    617\u001b[39m _validate_names(kwds.get(\u001b[33m\"\u001b[39m\u001b[33mnames\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[32m    619\u001b[39m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m620\u001b[39m parser = \u001b[43mTextFileReader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    622\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[32m    623\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/DS/venv/lib/python3.12/site-packages/pandas/io/parsers/readers.py:1620\u001b[39m, in \u001b[36mTextFileReader.__init__\u001b[39m\u001b[34m(self, f, engine, **kwds)\u001b[39m\n\u001b[32m   1617\u001b[39m     \u001b[38;5;28mself\u001b[39m.options[\u001b[33m\"\u001b[39m\u001b[33mhas_index_names\u001b[39m\u001b[33m\"\u001b[39m] = kwds[\u001b[33m\"\u001b[39m\u001b[33mhas_index_names\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m   1619\u001b[39m \u001b[38;5;28mself\u001b[39m.handles: IOHandles | \u001b[38;5;28;01mNone\u001b[39;00m = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1620\u001b[39m \u001b[38;5;28mself\u001b[39m._engine = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_make_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/DS/venv/lib/python3.12/site-packages/pandas/io/parsers/readers.py:1880\u001b[39m, in \u001b[36mTextFileReader._make_engine\u001b[39m\u001b[34m(self, f, engine)\u001b[39m\n\u001b[32m   1878\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mb\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[32m   1879\u001b[39m         mode += \u001b[33m\"\u001b[39m\u001b[33mb\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m-> \u001b[39m\u001b[32m1880\u001b[39m \u001b[38;5;28mself\u001b[39m.handles = \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1881\u001b[39m \u001b[43m    \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1882\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1883\u001b[39m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mencoding\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1884\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcompression\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1885\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmemory_map\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmemory_map\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1886\u001b[39m \u001b[43m    \u001b[49m\u001b[43mis_text\u001b[49m\u001b[43m=\u001b[49m\u001b[43mis_text\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1887\u001b[39m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mencoding_errors\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstrict\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1888\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstorage_options\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1889\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1890\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m.handles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1891\u001b[39m f = \u001b[38;5;28mself\u001b[39m.handles.handle\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/DS/venv/lib/python3.12/site-packages/pandas/io/common.py:873\u001b[39m, in \u001b[36mget_handle\u001b[39m\u001b[34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[39m\n\u001b[32m    868\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[32m    869\u001b[39m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[32m    870\u001b[39m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[32m    871\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m ioargs.encoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mb\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs.mode:\n\u001b[32m    872\u001b[39m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m873\u001b[39m         handle = \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[32m    874\u001b[39m \u001b[43m            \u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    875\u001b[39m \u001b[43m            \u001b[49m\u001b[43mioargs\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    876\u001b[39m \u001b[43m            \u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m=\u001b[49m\u001b[43mioargs\u001b[49m\u001b[43m.\u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    877\u001b[39m \u001b[43m            \u001b[49m\u001b[43merrors\u001b[49m\u001b[43m=\u001b[49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    878\u001b[39m \u001b[43m            \u001b[49m\u001b[43mnewline\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    879\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    880\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    881\u001b[39m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[32m    882\u001b[39m         handle = \u001b[38;5;28mopen\u001b[39m(handle, ioargs.mode)\n",
      "\u001b[31mFileNotFoundError\u001b[39m: [Errno 2] No such file or directory: 'train.csv'"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77de66bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "774031c7",
   "metadata": {},
   "source": [
    "df.shape → (1460, 81)\n",
    "\n",
    "1460 rows → each row is one house.\n",
    "\n",
    "81 columns → each column is a feature (like LotArea, YearBuilt) or the target (SalePrice)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "473d0982",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcde0c8c",
   "metadata": {},
   "source": [
    "Each row = a house column = a feature about the house Features can be: Numeric (LotArea = 8450, a number). Categorical (MSZoning = RL, a label). Missing (NaN = data not available). Target variable = SalePrice This is the column we’re trying to predict.\n",
    "\n",
    "The other 80 columns are the inputs to help predict it. Machine Learning’s job Learn patterns between features (like LotArea, OverallQual, YearBuilt) and the target SalePrice. Example: bigger lot → usually higher price, better quality rating → higher price."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7ae10bb",
   "metadata": {},
   "source": [
    "House Prices dataset has 3 files:\n",
    "\n",
    "train.csv → has features + SalePrice (target). Used for training/validation.\n",
    "\n",
    "test.csv → has features only (no SalePrice). Used for submission predictions.\n",
    "\n",
    "sample_submission.csv → shows the format you must submit: Id,SalePrice.\n",
    "\n",
    "So workflow is:\n",
    "\n",
    "Apply EDA + preprocessing + feature engineering only on train.csv.\n",
    "\n",
    "Train and validate models using the train.csv data.\n",
    "\n",
    "Once final model is ready → fit it on all of train.csv.\n",
    "\n",
    "Apply same preprocessing + feature engineering steps to test.csv.\n",
    "\n",
    "Predict SalePrice for test.csv.\n",
    "\n",
    "Reverse log transform (since we log1p-ed target):"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afe94809",
   "metadata": {},
   "source": [
    "2. Explore the dataset (EDA)\n",
    "\n",
    "This is to check:\n",
    "Number of rows and columns\n",
    "Data types\n",
    "Missing values\n",
    "Basic statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "775294d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e465219a",
   "metadata": {},
   "source": [
    "RangeIndex: 1460 entries, 0 to 1459\n",
    "→ The dataset has 1460 houses (rows).\n",
    "Data columns (total 81 columns)→ Each house has 81 details (features).\n",
    "Column types\n",
    "int64 → whole numbers (e.g. number of rooms, year built).\n",
    "float64 → decimals (e.g. lot frontage in feet, masonry veneer area).\n",
    "object → text data (e.g. neighborhood name, zoning, street type).\n",
    "Example:\n",
    "LotArea → int64, area of the lot in square feet.\n",
    "MSZoning → object, type of zoning (RL = residential low density).\n",
    "SalePrice → int64, the target we want to predict.\n",
    "Missing values (Non-Null Count)\n",
    "LotFrontage 1201 non-null out of 1460 → about 259 missing values.\n",
    "Alley 91 non-null → almost all missing (1369 missing).\n",
    "PoolQC 7 non-null → practically empty.\n",
    "FireplaceQu 770 non-null → about half missing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f730532c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe().T.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9680cd7a",
   "metadata": {},
   "source": [
    "Each row is a statistic, and each column is one of dataset’s numeric features.\n",
    "1. count\n",
    "Example: LotFrontage → 1201.000000\n",
    "Meaning: Only 1201 houses have this value (others are missing).\n",
    "Why important: Helps spot missing data.\n",
    "2. mean\n",
    "Example: LotArea → 10516.8\n",
    "Meaning: The average lot (land) size is about 10,516 square feet.\n",
    "Why important: Gives a sense of the \"center\" of the data.\n",
    "3. std (standard deviation)\n",
    "Example: SalePrice → 79,442\n",
    "Meaning: Prices are spread widely around the mean. Some houses are much cheaper,\n",
    "some much more expensive.\n",
    "Why important: Shows variability. High std = data is very spread out.\n",
    "4. min\n",
    "Example: YearBuilt → 1872\n",
    "Meaning: The oldest house was built in 1872.\n",
    "Why important: Shows the lower extreme in the dataset.\n",
    "5. 25% (first quartile, Q1)\n",
    "Example: OverallQual → 5.0\n",
    "Meaning: 25% of houses have an overall quality ≤ 5.\n",
    "Why important: Splits the data into four equal parts.\n",
    "6. 50% (median, Q2)\n",
    "Example: SalePrice → 163,000\n",
    "Meaning: Half of the houses cost less than 163k, half cost more.\n",
    "Why important: More robust than mean (not affected by very expensive houses).\n",
    "7. 75% (third quartile, Q3)\n",
    "Example: LotArea → 11,601.5\n",
    "Meaning: 75% of houses have lot area ≤ 11,601, and 25% have bigger land.\n",
    "Why important: Helps see where the “upper” bulk of the data lies.\n",
    "8. max\n",
    "Example: SalePrice → 755,000\n",
    "Meaning: The most expensive house sold for $755k.\n",
    "Why important: Spots outliers (extreme values)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f354a52c",
   "metadata": {},
   "source": [
    "Missing Values\n",
    "Check for columns with missing data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3546d2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isnull().sum().sort_values(ascending=False).head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1950ab92",
   "metadata": {},
   "source": [
    "PoolQC (pool quality) is missing in 1453 houses out of 1460 — almost nobody has a pool!\n",
    "Alley is missing in 1369 houses — meaning most don’t have an alley.\n",
    "Fence missing in 1179 houses — many houses don’t have fences.\n",
    "LotFrontage has 259 missing — here we might need to fill values.\n",
    "Some features like Garage have 81 missing (probably houses without a garage).\n",
    "Electrical has only 1 missing — easy to fix.\n",
    "Condition2 has 0 missing — perfect data.\n",
    "\n",
    "Columns with too many missing values (like PoolQC, Alley, Fence) may not be useful for prediction. Ist best to drop them."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c66b039b",
   "metadata": {},
   "source": [
    "##Drop columns with too many missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c1e817b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop columns with too many missing values\n",
    "df = df.drop(columns=[\"PoolQC\", \"MiscFeature\", \"Alley\", \"Fence\"])\n",
    "\n",
    "# Fill categorical NaNs with \"None\"\n",
    "for col in [\"GarageType\", \"GarageFinish\", \"GarageQual\", \"GarageCond\",\n",
    "            \"BsmtQual\", \"BsmtCond\", \"BsmtExposure\", \"BsmtFinType1\", \"BsmtFinType2\",\n",
    "            \"MasVnrType\"]:\n",
    "    df[col] = df[col].fillna(\"None\")\n",
    "\n",
    "# Fill numeric NaNs with median\n",
    "for col in [\"LotFrontage\", \"GarageYrBlt\", \"MasVnrArea\"]:\n",
    "    df[col] = df[col].fillna(df[col].median())\n",
    "\n",
    "# Fill Electrical with mode\n",
    "df[\"Electrical\"] = df[\"Electrical\"].fillna(df[\"Electrical\"].mode()[0])\n",
    "\n",
    "# Verify no missing values remain\n",
    "print(df.isnull().sum().sort_values(ascending=False).head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd41d96f",
   "metadata": {},
   "source": [
    "A number greater than 0 = that many missing values remain.\n",
    "A 0 = clean column, no missing data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fd637ab",
   "metadata": {},
   "source": [
    "Univariate Analysis;\n",
    "\n",
    "Look at the target (SalePrice) distribution.\n",
    "Explore each feature individually (numeric + categorical).\n",
    "Spot missing values, skewness, and outliers.\n",
    "\n",
    "This step makes sure you understand your data’s shape before comparing.\n",
    "\n",
    "Bivariate Analysis (second)\n",
    "Compare each feature against SalePrice.\n",
    "Numeric → scatterplots & correlation heatmap.\n",
    "Categorical → boxplots or violin plots vs SalePrice.\n",
    "\n",
    "This step tells you which features are useful for prediction.\n",
    "\n",
    "Feature Engineering\n",
    "Fix missing values.\n",
    "Handle skewness (log transform SalePrice, etc.).\n",
    "Create new features (e.g., total area = basement + living area).\n",
    "Model Building & Evaluation\n",
    "Split into train/test.\n",
    "\n",
    "Try baseline model (Linear Regression).\n",
    "Evaluate with RMSE, R².\n",
    "Improve with advanced models (Random Forest, XGBoost)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69319ef0",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8,6))\n",
    "sns.histplot(df['SalePrice'], kde=True, bins=30)\n",
    "plt.title(\"Distribution of SalePrice\")\n",
    "plt.xlabel(\"Sale Price\")\n",
    "plt.ylabel(\"Count\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6945a0fc",
   "metadata": {},
   "source": [
    "This shows how house prices are distributed.\n",
    "Most houses fall in a central range, but you may see a long right tail (skewness).\n",
    "If very skewed, we might need a log transform later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "117bba19",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8,4))\n",
    "sns.boxplot(x=df['SalePrice'])\n",
    "plt.title(\"Boxplot of SalePrice\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c23ffe69",
   "metadata": {},
   "source": [
    "Shows spread + potential outliers (very expensive houses).\n",
    "Outliers can heavily affect models like Linear Regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8e7e05d",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(6,4))\n",
    "sns.countplot(x=df['MSZoning'])\n",
    "plt.title(\"Count of Houses by Zoning Category\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0171365d",
   "metadata": {},
   "source": [
    "Shows how many houses fall into each zoning category.\n",
    "Helps us detect imbalances (e.g., most houses may belong to \"RL\")."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dcdbb08",
   "metadata": {},
   "outputs": [],
   "source": [
    "##Bivariate Analysis\n",
    "\n",
    "#Compare features against SalePrice (target). This shows relationships and predictive power.\n",
    "\n",
    "\n",
    "plt.figure(figsize=(12,8))\n",
    "corr = df.corr(numeric_only=True)  # only numeric columns\n",
    "sns.heatmap(corr, cmap=\"coolwarm\", center=0)\n",
    "plt.title(\"Correlation Heatmap\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d50b09d",
   "metadata": {},
   "source": [
    "Correlation ranges from -1 to +1.\n",
    "+1 → strong positive (as feature ↑, price ↑).\n",
    "-1 → strong negative (as feature ↑, price ↓).\n",
    "0 → no linear relationship."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33a59cca",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8,6))\n",
    "sns.scatterplot(x=df['GrLivArea'], y=df['SalePrice'])\n",
    "plt.title(\"Living Area vs SalePrice\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "110f5f2b",
   "metadata": {},
   "source": [
    "Each dot = one house.\n",
    "Bigger houses (GrLivArea) → usually higher prices.\n",
    "Watch for outliers: huge houses with low prices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36b2b6aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8,6))\n",
    "sns.boxplot(x=df['OverallQual'], y=df['SalePrice'])\n",
    "plt.title(\"House Quality vs SalePrice\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46ac7ead",
   "metadata": {},
   "source": [
    "OverallQual is an ordinal rating (1–10).\n",
    "Higher quality → higher sale price.\n",
    "Shows clear upward trend."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edebc8bc",
   "metadata": {},
   "source": [
    "Univariate → understand each variable separately.\n",
    "SalePrice distribution → skewness & outliers.\n",
    "Boxplots → highlight extreme values.\n",
    "Countplots → category balance.\n",
    "\n",
    "Bivariate → check feature vs SalePrice.\n",
    "Heatmap → numeric correlations.\n",
    "Scatterplots → relationships + outliers.\n",
    "Boxplots → how categories relate to SalePrice"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6a0f3bd",
   "metadata": {},
   "source": [
    "# Baseline Model: Linear Regression with Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9ce69ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "import joblib\n",
    "\n",
    "# --- Step 1: Load raw data ---\n",
    "train = pd.read_csv(\"train.csv\")\n",
    "test = pd.read_csv(\"test.csv\")\n",
    "\n",
    "# Separate features/target\n",
    "X = train.drop([\"SalePrice\", \"Id\"], axis=1).copy()\n",
    "y = np.log1p(train[\"SalePrice\"]).copy()   # log-transform target\n",
    "\n",
    "# Save test Ids for submission\n",
    "test_ids = test[\"Id\"]\n",
    "X_test_final = test.drop(\"Id\", axis=1).copy()\n",
    "\n",
    "# Train/validation split\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# --- Step 2: Identify numeric & categorical columns ---\n",
    "num_cols = X_train.select_dtypes(include=[\"int64\", \"float64\"]).columns.tolist()\n",
    "cat_cols = X_train.select_dtypes(include=[\"object\"]).columns.tolist()\n",
    "\n",
    "# --- Step 3: Pipelines for preprocessing ---\n",
    "num_pipeline = Pipeline([\n",
    "    (\"imputer\", SimpleImputer(strategy=\"median\")),\n",
    "    (\"scaler\", StandardScaler())\n",
    "])\n",
    "\n",
    "cat_pipeline = Pipeline([\n",
    "    (\"imputer\", SimpleImputer(strategy=\"most_frequent\")),\n",
    "    (\"onehot\", OneHotEncoder(handle_unknown=\"ignore\", sparse_output=False))\n",
    "])\n",
    "\n",
    "preprocessor = ColumnTransformer([\n",
    "    (\"num\", num_pipeline, num_cols),\n",
    "    (\"cat\", cat_pipeline, cat_cols)\n",
    "])\n",
    "\n",
    "# --- Step 4: Build Linear Regression pipeline ---\n",
    "pipeline = Pipeline([\n",
    "    (\"preprocessor\", preprocessor),\n",
    "    (\"regressor\", LinearRegression())\n",
    "])\n",
    "\n",
    "# --- Step 5: Cross-validation ---\n",
    "cv_scores = cross_val_score(pipeline, X, y, cv=5, scoring=\"r2\")\n",
    "print(\"Cross-validation R² scores:\", cv_scores)\n",
    "print(\"Mean CV R²:\", cv_scores.mean())\n",
    "\n",
    "# --- Step 6: Validation check ---\n",
    "pipeline.fit(X_train, y_train)\n",
    "y_val_pred = pipeline.predict(X_val)\n",
    "\n",
    "val_rmse = np.sqrt(mean_squared_error(y_val, y_val_pred))\n",
    "val_r2 = r2_score(y_val, y_val_pred)\n",
    "print(f\"Validation RMSE (log-target): {val_rmse:.4f}, R²: {val_r2:.4f}\")\n",
    "\n",
    "# --- Step 7: Final test predictions ---\n",
    "test_preds = np.expm1(pipeline.predict(X_test_final))   # reverse log-transform\n",
    "\n",
    "submission = pd.DataFrame({\n",
    "    \"Id\": test_ids,\n",
    "    \"SalePrice\": test_preds\n",
    "})\n",
    "submission.to_csv(\"submission.csv\", index=False)\n",
    "print(\"Submission file created: submission.csv\")\n",
    "\n",
    "# --- Step 8: Save model ---\n",
    "joblib.dump(pipeline, \"linear_regression_baseline.pkl\")\n",
    "print(\" Model pipeline saved: linear_regression_baseline.pkl\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08e4343d",
   "metadata": {},
   "source": [
    "Tools for Building & Testing Models\n",
    "\n",
    "joblib → Save & load models to disk so you don’t retrain every time.\n",
    "train_test_split → Splits data into training (to learn) and test/validation (to check performance).\n",
    "cross_val_score → Runs cross-validation, testing the model on multiple folds to check reliability.\n",
    "\n",
    "Preprocessing Pipelines\n",
    "Pipeline → Chains steps (e.g., impute missing → scale → train model) into one object.\n",
    "ColumnTransformer → Lets you apply different preprocessing to numeric and categorical features in one go.\n",
    "Common Transformers\n",
    "SimpleImputer → Fills in missing values.\n",
    "Example: replace missing numbers with the median.\n",
    "StandardScaler → Standardizes numeric features (mean = 0, std = 1).\n",
    "OneHotEncoder → Converts categories (like \"A\", \"B\", \"C\") into binary 0/1 columns.\n",
    "Models\n",
    "\n",
    "LinearRegression → A basic regression model that learns a straight-line relationship between features and target.\n",
    "Evaluation Metrics\n",
    "mean_squared_error (MSE) → Average of squared errors (penalizes large mistakes).\n",
    "mean_absolute_error (MAE) → Average of absolute errors (easier to interpret).\n",
    "r2_score (R²) → How much variation the model explains (1.0 = perfect fit)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56b1454c",
   "metadata": {},
   "source": [
    "# Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "470ec64f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def feature_engineering(df):\n",
    "    df = df.copy()\n",
    "\n",
    "    # ----- 1. Create new features -----\n",
    "    df[\"TotalSF\"] = df[\"1stFlrSF\"] + df[\"2ndFlrSF\"] + df[\"TotalBsmtSF\"]\n",
    "    df[\"TotalBath\"] = (df[\"FullBath\"] + 0.5*df[\"HalfBath\"] + \n",
    "                       df[\"BsmtFullBath\"] + 0.5*df[\"BsmtHalfBath\"])\n",
    "    df[\"PorchArea\"] = (df[\"OpenPorchSF\"] + df[\"EnclosedPorch\"] + \n",
    "                       df[\"3SsnPorch\"] + df[\"ScreenPorch\"])\n",
    "\n",
    "    # ----- 2. Encode ordinal quality features -----\n",
    "    qual_mapping = {\"Ex\":5, \"Gd\":4, \"TA\":3, \"Fa\":2, \"Po\":1, np.nan:0}\n",
    "    ordinal_cols = [\"ExterQual\", \"KitchenQual\", \"BsmtQual\", \n",
    "                    \"HeatingQC\", \"FireplaceQu\", \"GarageQual\"]\n",
    "    for col in ordinal_cols:\n",
    "        if col in df.columns:\n",
    "            df[col] = df[col].map(qual_mapping)\n",
    "\n",
    "    # ----- 3. Handle skewed numeric features -----\n",
    "    numeric_feats = df.select_dtypes(include=[\"int64\",\"float64\"])\n",
    "    skewness = numeric_feats.apply(lambda x: x.dropna().skew()).sort_values(ascending=False)\n",
    "    skewed_cols = skewness[skewness > 0.75].index\n",
    "    df[skewed_cols] = np.log1p(df[skewed_cols])\n",
    "\n",
    "    return df\n",
    "# Load data\n",
    "train = pd.read_csv(\"train.csv\")\n",
    "test = pd.read_csv(\"test.csv\")\n",
    "\n",
    "# Apply feature engineering\n",
    "train_fe = feature_engineering(train)\n",
    "test_fe = feature_engineering(test)\n",
    "\n",
    "# Log-transform SalePrice only in train\n",
    "y = np.log1p(train_fe[\"SalePrice\"])\n",
    "X = train_fe.drop(\"SalePrice\", axis=1)\n",
    "\n",
    "# Keep test features ready\n",
    "X_test_final = test_fe\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7007b778",
   "metadata": {},
   "source": [
    "Key Points:\n",
    "\n",
    "Both train and test go through the same feature engineering function.\n",
    "\n",
    "Only the train set’s SalePrice gets log-transformed."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb36355b",
   "metadata": {},
   "source": [
    "# Split + Preprocessing Pipelines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7454d5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "import joblib\n",
    "\n",
    "# --- Step 0: Load data ---\n",
    "train = pd.read_csv(\"train.csv\")\n",
    "test = pd.read_csv(\"test.csv\")\n",
    "\n",
    "# Save test IDs separately\n",
    "test_id = test[\"Id\"].copy()\n",
    "\n",
    "# Drop Id from train and test\n",
    "train.drop(\"Id\", axis=1, inplace=True)\n",
    "test.drop(\"Id\", axis=1, inplace=True)\n",
    "\n",
    "# --- Step 1: Separate target & features ---\n",
    "y = np.log1p(train[\"SalePrice\"])   # log-transform SalePrice\n",
    "X = train.drop(\"SalePrice\", axis=1)\n",
    "\n",
    "# --- Step 2: Train/validation split ---\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# --- Step 3: Identify numeric & categorical columns ---\n",
    "num_cols = X_train.select_dtypes(include=[\"int64\", \"float64\"]).columns.tolist()\n",
    "cat_cols = X_train.select_dtypes(include=[\"object\"]).columns.tolist()\n",
    "\n",
    "# --- Step 4: Preprocessing pipelines ---\n",
    "num_pipeline = Pipeline([\n",
    "    (\"imputer\", SimpleImputer(strategy=\"median\")),\n",
    "    (\"scaler\", StandardScaler())\n",
    "])\n",
    "\n",
    "cat_pipeline = Pipeline([\n",
    "    (\"imputer\", SimpleImputer(strategy=\"most_frequent\")),\n",
    "    (\"onehot\", OneHotEncoder(handle_unknown=\"ignore\", sparse_output=False))\n",
    "])\n",
    "\n",
    "preprocessor = ColumnTransformer([\n",
    "    (\"num\", num_pipeline, num_cols),\n",
    "    (\"cat\", cat_pipeline, cat_cols)\n",
    "])\n",
    "\n",
    "# --- Step 5: Build Linear Regression pipeline ---\n",
    "pipeline = Pipeline([\n",
    "    (\"preprocessor\", preprocessor),\n",
    "    (\"regressor\", LinearRegression())\n",
    "])\n",
    "\n",
    "# --- Step 6: Cross-validation ---\n",
    "cv_scores = cross_val_score(pipeline, X, y, cv=5, scoring=\"r2\")\n",
    "print(\"Cross-validation R² scores:\", cv_scores)\n",
    "print(\"Mean CV R²:\", cv_scores.mean())\n",
    "\n",
    "# --- Step 7: Fit & validate ---\n",
    "pipeline.fit(X_train, y_train)\n",
    "y_val_pred = pipeline.predict(X_val)\n",
    "\n",
    "val_rmse = np.sqrt(mean_squared_error(y_val, y_val_pred))\n",
    "val_r2 = r2_score(y_val, y_val_pred)\n",
    "\n",
    "print(f\"Validation RMSE (log-target): {val_rmse:.4f}, R²: {val_r2:.4f}\")\n",
    "\n",
    "# --- Step 8: Final predictions on test set ---\n",
    "test_preds = np.expm1(pipeline.predict(test))  # reverse log1p\n",
    "\n",
    "submission = pd.DataFrame({\n",
    "    \"Id\": test_id,\n",
    "    \"SalePrice\": test_preds\n",
    "})\n",
    "submission.to_csv(\"submission.csv\", index=False)\n",
    "print(\"Submission file created: submission.csv\")\n",
    "\n",
    "# --- Step 9: Save model ---\n",
    "joblib.dump(pipeline, \"linear_regression_pipeline.pkl\")\n",
    "print(\"Model pipeline saved: linear_regression_pipeline.pkl\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16cdfe54",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "\n",
    "# --- Step 1: Split train/validation ---\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# --- Step 2: Identify numeric & categorical columns ---\n",
    "num_cols = X_train.select_dtypes(include=[\"int64\", \"float64\"]).columns.tolist()\n",
    "cat_cols = X_train.select_dtypes(include=[\"object\"]).columns.tolist()\n",
    "\n",
    "# --- Step 3: Pipelines ---\n",
    "num_pipeline = Pipeline([\n",
    "    (\"imputer\", SimpleImputer(strategy=\"median\")),\n",
    "    (\"scaler\", StandardScaler())\n",
    "])\n",
    "\n",
    "cat_pipeline = Pipeline([\n",
    "    (\"imputer\", SimpleImputer(strategy=\"most_frequent\")),\n",
    "    (\"onehot\", OneHotEncoder(handle_unknown=\"ignore\", sparse_output=False))\n",
    "])\n",
    "\n",
    "# Combine numeric + categorical preprocessing\n",
    "preprocessor = ColumnTransformer([\n",
    "    (\"num\", num_pipeline, num_cols),\n",
    "    (\"cat\", cat_pipeline, cat_cols)\n",
    "])\n",
    "\n",
    "# --- Apply preprocessing on train & validation sets ---\n",
    "X_train_processed = preprocessor.fit_transform(X_train)\n",
    "X_val_processed = preprocessor.transform(X_val)\n",
    "\n",
    "# Convert the transformed arrays back into a DataFrame\n",
    "# (so you can see columns instead of just numbers)\n",
    "# First, get the names of all transformed features:\n",
    "cat_feature_names = preprocessor.named_transformers_[\"cat\"][\"onehot\"].get_feature_names_out(cat_cols)\n",
    "all_feature_names = num_cols + cat_feature_names.tolist()\n",
    "\n",
    "# Wrap in a DataFrame\n",
    "X_train_df = pd.DataFrame(X_train_processed, columns=all_feature_names, index=X_train.index)\n",
    "X_val_df = pd.DataFrame(X_val_processed, columns=all_feature_names, index=X_val.index)\n",
    "\n",
    "# Print shapes\n",
    "print(\"Original train shape:\", X_train.shape)\n",
    "print(\"Transformed train shape:\", X_train_df.shape)\n",
    "\n",
    "# Show a preview\n",
    "print(X_train_df.head())\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1e13eb2",
   "metadata": {},
   "source": [
    "# Linear Regression Pipeline (with Evaluation + Test Predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dbacbd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "import joblib\n",
    "\n",
    "# --- Step 1: Read CSV ---\n",
    "train = pd.read_csv(\"train.csv\")\n",
    "test = pd.read_csv(\"test.csv\")\n",
    "\n",
    "# Keep test IDs safe\n",
    "test_ids = test[\"Id\"].copy()\n",
    "\n",
    "# Drop Id from both sets\n",
    "train.drop(\"Id\", axis=1, inplace=True)\n",
    "test.drop(\"Id\", axis=1, inplace=True)\n",
    "\n",
    "# Separate target from features\n",
    "y = train[\"SalePrice\"]\n",
    "X = train.drop(\"SalePrice\", axis=1)\n",
    "\n",
    "# --- Step 1b: Train/validation split ---\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# --- Step 2: Numeric & categorical columns ---\n",
    "num_cols = X_train.select_dtypes(include=[\"int64\", \"float64\"]).columns.tolist()\n",
    "cat_cols = X_train.select_dtypes(include=[\"object\"]).columns.tolist()\n",
    "\n",
    "# --- Step 3: Pipelines ---\n",
    "num_pipeline = Pipeline([\n",
    "    (\"imputer\", SimpleImputer(strategy=\"median\")),\n",
    "    (\"scaler\", StandardScaler())\n",
    "])\n",
    "\n",
    "cat_pipeline = Pipeline([\n",
    "    (\"imputer\", SimpleImputer(strategy=\"most_frequent\")),\n",
    "    (\"onehot\", OneHotEncoder(handle_unknown=\"ignore\", sparse_output=False))\n",
    "])\n",
    "\n",
    "preprocessor = ColumnTransformer([\n",
    "    (\"num\", num_pipeline, num_cols),\n",
    "    (\"cat\", cat_pipeline, cat_cols)\n",
    "])\n",
    "\n",
    "# --- Step 4: Build Linear Regression pipeline ---\n",
    "pipeline = Pipeline([\n",
    "    (\"preprocessor\", preprocessor),\n",
    "    (\"regressor\", LinearRegression())\n",
    "])\n",
    "\n",
    "# --- Step 5: Cross-validation (R² scores) ---\n",
    "cv_scores = cross_val_score(pipeline, X, y, cv=5, scoring=\"r2\")\n",
    "print(\"Cross-validation R² scores:\", cv_scores)\n",
    "print(\"Mean CV R²:\", cv_scores.mean())\n",
    "\n",
    "# --- Step 6: Fit & validate ---\n",
    "pipeline.fit(X_train, y_train)\n",
    "y_val_pred = pipeline.predict(X_val)\n",
    "\n",
    "val_rmse = np.sqrt(mean_squared_error(y_val, y_val_pred))\n",
    "val_r2 = r2_score(y_val, y_val_pred)\n",
    "\n",
    "print(f\"Validation RMSE: {val_rmse:.4f}, R²: {val_r2:.4f}\")\n",
    "\n",
    "# --- Step 7: Final predictions on test set ---\n",
    "X_test_final = test.copy()  # FIX: use test, not undefined 'test_fe'\n",
    "\n",
    "# FIX: Ensure test has same columns as training\n",
    "missing_cols = set(X_train.columns) - set(X_test_final.columns)\n",
    "for c in missing_cols:\n",
    "    X_test_final[c] = 0  # numeric default, imputer can handle NaNs\n",
    "\n",
    "# Reorder columns to match X_train\n",
    "X_test_final = X_test_final[X_train.columns]\n",
    "\n",
    "# Predict (FIX: remove np.expm1, only needed if y was log-transformed)\n",
    "test_preds = pipeline.predict(X_test_final)\n",
    "\n",
    "# Create submission\n",
    "submission = pd.DataFrame({\n",
    "    \"Id\": test_ids,\n",
    "    \"SalePrice\": test_preds\n",
    "})\n",
    "submission.to_csv(\"submission.csv\", index=False)\n",
    "print(\"Submission file created: submission.csv\")\n",
    "\n",
    "# --- Step 8: Save model ---\n",
    "joblib.dump(pipeline, \"linear_regression_pipeline.pkl\")\n",
    "print(\"Model pipeline saved: linear_regression_pipeline.pkl\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5450d3f9",
   "metadata": {},
   "source": [
    "Splits training into train/validation.\n",
    "\n",
    "Builds preprocessing for numeric & categorical features.\n",
    "\n",
    "Trains a Linear Regression model.\n",
    "\n",
    "Reports:\n",
    "\n",
    "Cross-validation R²\n",
    "\n",
    "Validation RMSE (log-target)\n",
    "\n",
    "Predicts test set & saves submission.csv.\n",
    "\n",
    "Saves trained pipeline as linear_regression_pipeline.pkl."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba32e459",
   "metadata": {},
   "source": [
    "# Ridge Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c69df00",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "import joblib\n",
    "\n",
    "# --- Step 0: Read CSV ---\n",
    "train = pd.read_csv(\"train.csv\")\n",
    "test = pd.read_csv(\"test.csv\")\n",
    "\n",
    "# Keep test IDs safe\n",
    "test_ids = test[\"Id\"].copy()\n",
    "\n",
    "# Drop 'Id' from both sets\n",
    "train.drop(\"Id\", axis=1, inplace=True)\n",
    "test.drop(\"Id\", axis=1, inplace=True)\n",
    "\n",
    "# Separate target and features\n",
    "y = train[\"SalePrice\"]\n",
    "X = train.drop(\"SalePrice\", axis=1)\n",
    "\n",
    "# --- Step 1: Train/validation split ---\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# --- Step 2: Numeric & categorical columns ---\n",
    "num_cols = X_train.select_dtypes(include=[\"int64\", \"float64\"]).columns.tolist()\n",
    "cat_cols = X_train.select_dtypes(include=[\"object\"]).columns.tolist()\n",
    "\n",
    "# --- Step 3: Pipelines ---\n",
    "num_pipeline = Pipeline([\n",
    "    (\"imputer\", SimpleImputer(strategy=\"median\")),\n",
    "    (\"scaler\", StandardScaler())\n",
    "])\n",
    "\n",
    "cat_pipeline = Pipeline([\n",
    "    (\"imputer\", SimpleImputer(strategy=\"most_frequent\")),\n",
    "    (\"onehot\", OneHotEncoder(handle_unknown=\"ignore\", sparse_output=False))\n",
    "])\n",
    "\n",
    "preprocessor = ColumnTransformer([\n",
    "    (\"num\", num_pipeline, num_cols),\n",
    "    (\"cat\", cat_pipeline, cat_cols)\n",
    "])\n",
    "\n",
    "# --- Step 4: Ridge pipeline ---\n",
    "ridge = Ridge(alpha=10, random_state=42)\n",
    "\n",
    "ridge_pipeline = Pipeline([\n",
    "    (\"preprocessor\", preprocessor),\n",
    "    (\"model\", ridge)\n",
    "])\n",
    "\n",
    "# --- Step 5: Cross-validation (RMSE) ---\n",
    "# Use negative RMSE scoring\n",
    "ridge_cv_scores = -cross_val_score(\n",
    "    ridge_pipeline, X_train, y_train, cv=5, scoring=\"neg_root_mean_squared_error\"\n",
    ")\n",
    "print(\"Ridge CV RMSE:\", ridge_cv_scores.mean())\n",
    "\n",
    "# --- Step 6: Fit on training data ---\n",
    "ridge_pipeline.fit(X_train, y_train)\n",
    "\n",
    "# --- Step 7: Validation performance ---\n",
    "y_val_pred_ridge = ridge_pipeline.predict(X_val)\n",
    "ridge_rmse = np.sqrt(mean_squared_error(y_val, y_val_pred_ridge))\n",
    "ridge_r2 = r2_score(y_val, y_val_pred_ridge)\n",
    "print(f\"Ridge Validation RMSE: {ridge_rmse:.4f}, R²: {ridge_r2:.4f}\")\n",
    "\n",
    "# --- Step 8: Prepare test set for prediction ---\n",
    "X_test_final = test.copy()  # use test DataFrame\n",
    "\n",
    "# Ensure test columns match training columns\n",
    "missing_cols = set(X_train.columns) - set(X_test_final.columns)\n",
    "for c in missing_cols:\n",
    "    X_test_final[c] = 0  # or np.nan, SimpleImputer will handle it\n",
    "\n",
    "# Reorder columns to match training\n",
    "X_test_final = X_test_final[X_train.columns]\n",
    "\n",
    "# --- Step 9: Predict on test set ---\n",
    "ridge_test_preds = ridge_pipeline.predict(X_test_final)\n",
    "\n",
    "# --- Step 10: Create submission ---\n",
    "ridge_submission = pd.DataFrame({\n",
    "    \"Id\": test_ids,\n",
    "    \"SalePrice\": ridge_test_preds\n",
    "})\n",
    "ridge_submission.to_csv(\"ridge_submission.csv\", index=False)\n",
    "print(\"Ridge submission saved!\")\n",
    "\n",
    "# --- Step 11: Save Ridge model pipeline ---\n",
    "joblib.dump(ridge_pipeline, \"ridge_pipeline.pkl\")\n",
    "print(\"Ridge model pipeline saved!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfc11a14",
   "metadata": {},
   "source": [
    "# Lasso Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60688257",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "\n",
    "# --- Step 1: Build Lasso pipeline ---\n",
    "lasso = Lasso(alpha=0.001, random_state=42, max_iter=10000)\n",
    "lasso_pipeline = Pipeline([\n",
    "    (\"preprocessor\", preprocessor),  # reuse your preprocessor from previous code\n",
    "    (\"model\", lasso)\n",
    "])\n",
    "\n",
    "# --- Step 2: Fit on training data ---\n",
    "lasso_pipeline.fit(X_train, y_train)\n",
    "\n",
    "# --- Step 3: Validation performance ---\n",
    "y_val_pred = lasso_pipeline.predict(X_val)\n",
    "\n",
    "# FIX: Compute RMSE manually without `squared` argument\n",
    "lasso_rmse = np.sqrt(mean_squared_error(y_val, y_val_pred))\n",
    "lasso_r2 = r2_score(y_val, y_val_pred)\n",
    "\n",
    "print(f\"Lasso RMSE: {lasso_rmse:.4f}, R²: {lasso_r2:.4f}\")\n",
    "\n",
    "# --- Step 4: Prepare test set for prediction ---\n",
    "X_test_final = test.copy()\n",
    "\n",
    "# Ensure test columns match training columns\n",
    "missing_cols = set(X_train.columns) - set(X_test_final.columns)\n",
    "for c in missing_cols:\n",
    "    X_test_final[c] = 0  # or np.nan, SimpleImputer will handle it\n",
    "\n",
    "# Reorder columns to match training\n",
    "X_test_final = X_test_final[X_train.columns]\n",
    "\n",
    "# --- Step 5: Predict on test set ---\n",
    "lasso_test_preds = lasso_pipeline.predict(X_test_final)\n",
    "\n",
    "# --- Step 6: Create submission ---\n",
    "lasso_submission = pd.DataFrame({\n",
    "    \"Id\": test_ids,\n",
    "    \"SalePrice\": lasso_test_preds\n",
    "})\n",
    "lasso_submission.to_csv(\"lasso_submission.csv\", index=False)\n",
    "print(\"Lasso submission saved!\")\n",
    "\n",
    "# --- Step 7: Save Lasso model pipeline ---\n",
    "import joblib\n",
    "joblib.dump(lasso_pipeline, \"lasso_pipeline.pkl\")\n",
    "print(\"Lasso model pipeline saved!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a94fdd80",
   "metadata": {},
   "source": [
    "# Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfee620a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "import joblib\n",
    "\n",
    "# --- Step 0: Read data ---\n",
    "train = pd.read_csv(\"train.csv\")\n",
    "test = pd.read_csv(\"test.csv\")\n",
    "\n",
    "# Keep test IDs\n",
    "test_ids = test[\"Id\"].copy()\n",
    "\n",
    "# Drop Id\n",
    "train.drop(\"Id\", axis=1, inplace=True)\n",
    "test.drop(\"Id\", axis=1, inplace=True)\n",
    "\n",
    "# Separate target\n",
    "y = train[\"SalePrice\"]\n",
    "X = train.drop(\"SalePrice\", axis=1)\n",
    "\n",
    "# --- Step 1: Train/validation split ---\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# --- Step 2: Preprocessing ---\n",
    "num_cols = X_train.select_dtypes(include=[\"int64\", \"float64\"]).columns.tolist()\n",
    "cat_cols = X_train.select_dtypes(include=[\"object\"]).columns.tolist()\n",
    "\n",
    "num_pipeline = Pipeline([\n",
    "    (\"imputer\", SimpleImputer(strategy=\"median\")),\n",
    "    (\"scaler\", StandardScaler())\n",
    "])\n",
    "\n",
    "cat_pipeline = Pipeline([\n",
    "    (\"imputer\", SimpleImputer(strategy=\"most_frequent\")),\n",
    "    (\"onehot\", OneHotEncoder(handle_unknown=\"ignore\", sparse_output=False))\n",
    "])\n",
    "\n",
    "preprocessor = ColumnTransformer([\n",
    "    (\"num\", num_pipeline, num_cols),\n",
    "    (\"cat\", cat_pipeline, cat_cols)\n",
    "])\n",
    "\n",
    "# --- Step 3: Random Forest pipeline ---\n",
    "rf = RandomForestRegressor(\n",
    "    n_estimators=500, max_depth=None, random_state=42, n_jobs=-1\n",
    ")\n",
    "rf_pipeline = Pipeline([\n",
    "    (\"preprocessor\", preprocessor),\n",
    "    (\"model\", rf)\n",
    "])\n",
    "\n",
    "# Fit and validate\n",
    "rf_pipeline.fit(X_train, y_train)\n",
    "y_val_pred_rf = rf_pipeline.predict(X_val)\n",
    "rf_rmse = np.sqrt(mean_squared_error(y_val, y_val_pred_rf))\n",
    "rf_r2 = r2_score(y_val, y_val_pred_rf)\n",
    "print(f\"Random Forest RMSE: {rf_rmse:.4f}, R²: {rf_r2:.4f}\")\n",
    "\n",
    "# --- Step 4: Prepare test set ---\n",
    "X_test_final = test.copy()\n",
    "missing_cols = set(X_train.columns) - set(X_test_final.columns)\n",
    "for c in missing_cols:\n",
    "    X_test_final[c] = 0\n",
    "X_test_final = X_test_final[X_train.columns]\n",
    "\n",
    "# Predict test\n",
    "rf_test_preds = rf_pipeline.predict(X_test_final)\n",
    "\n",
    "# Submission\n",
    "rf_submission = pd.DataFrame({\n",
    "    \"Id\": test_ids,\n",
    "    \"SalePrice\": rf_test_preds\n",
    "})\n",
    "rf_submission.to_csv(\"rf_submission.csv\", index=False)\n",
    "print(\"Random Forest submission saved!\")\n",
    "\n",
    "# Save pipeline\n",
    "joblib.dump(rf_pipeline, \"rf_pipeline.pkl\")\n",
    "print(\"Random Forest pipeline saved!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26a55e42",
   "metadata": {},
   "source": [
    "# XGBoost"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80ffb21f",
   "metadata": {},
   "source": [
    "# combined script with Linear Regression, Ridge, Lasso, Random Forest, and XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebf5bd74",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.linear_model import LinearRegression, Ridge, Lasso\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "import joblib\n",
    "\n",
    "# Optional: XGBoost\n",
    "try:\n",
    "    from xgboost import XGBRegressor\n",
    "    xgb_installed = True\n",
    "except ModuleNotFoundError:\n",
    "    print(\"XGBoost not installed. Skipping XGBoost.\")\n",
    "    xgb_installed = False\n",
    "\n",
    "# --- Step 0: Read data ---\n",
    "train = pd.read_csv(\"train.csv\")\n",
    "test = pd.read_csv(\"test.csv\")\n",
    "test_ids = test[\"Id\"].copy()\n",
    "\n",
    "train.drop(\"Id\", axis=1, inplace=True)\n",
    "test.drop(\"Id\", axis=1, inplace=True)\n",
    "\n",
    "y = train[\"SalePrice\"]\n",
    "X = train.drop(\"SalePrice\", axis=1)\n",
    "\n",
    "# --- Step 1: Train/validation split ---\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# --- Step 2: Preprocessing ---\n",
    "num_cols = X_train.select_dtypes(include=[\"int64\", \"float64\"]).columns.tolist()\n",
    "cat_cols = X_train.select_dtypes(include=[\"object\"]).columns.tolist()\n",
    "\n",
    "num_pipeline = Pipeline([\n",
    "    (\"imputer\", SimpleImputer(strategy=\"median\")),\n",
    "    (\"scaler\", StandardScaler())\n",
    "])\n",
    "\n",
    "cat_pipeline = Pipeline([\n",
    "    (\"imputer\", SimpleImputer(strategy=\"most_frequent\")),\n",
    "    (\"onehot\", OneHotEncoder(handle_unknown=\"ignore\", sparse_output=False))\n",
    "])\n",
    "\n",
    "preprocessor = ColumnTransformer([\n",
    "    (\"num\", num_pipeline, num_cols),\n",
    "    (\"cat\", cat_pipeline, cat_cols)\n",
    "])\n",
    "\n",
    "# --- Step 3: Models ---\n",
    "models = {\n",
    "    \"LinearRegression\": LinearRegression(),\n",
    "    \"Ridge\": Ridge(alpha=10, random_state=42),\n",
    "    \"Lasso\": Lasso(alpha=0.001, random_state=42, max_iter=10000),\n",
    "    \"RandomForest\": RandomForestRegressor(n_estimators=500, max_depth=None, random_state=42, n_jobs=-1)\n",
    "}\n",
    "\n",
    "if xgb_installed:\n",
    "    models[\"XGBoost\"] = XGBRegressor(\n",
    "        n_estimators=1000,\n",
    "        learning_rate=0.05,\n",
    "        max_depth=4,\n",
    "        subsample=0.8,\n",
    "        colsample_bytree=0.8,\n",
    "        random_state=42\n",
    "    )\n",
    "\n",
    "# --- Step 4: Train, evaluate, and save each model ---\n",
    "for name, model in models.items():\n",
    "    print(f\"\\n{name} training...\")\n",
    "    pipeline = Pipeline([\n",
    "        (\"preprocessor\", preprocessor),\n",
    "        (\"model\", model)\n",
    "    ])\n",
    "    \n",
    "    # Fit\n",
    "    pipeline.fit(X_train, y_train)\n",
    "    \n",
    "    # Validation\n",
    "    y_val_pred = pipeline.predict(X_val)\n",
    "    rmse = np.sqrt(mean_squared_error(y_val, y_val_pred))\n",
    "    r2 = r2_score(y_val, y_val_pred)\n",
    "    print(f\"{name} Validation RMSE: {rmse:.4f}, R²: {r2:.4f}\")\n",
    "    \n",
    "    # Prepare test set\n",
    "    X_test_final = test.copy()\n",
    "    missing_cols = set(X_train.columns) - set(X_test_final.columns)\n",
    "    for c in missing_cols:\n",
    "        X_test_final[c] = 0\n",
    "    X_test_final = X_test_final[X_train.columns]\n",
    "    \n",
    "    # Predict test\n",
    "    test_preds = pipeline.predict(X_test_final)\n",
    "    \n",
    "    # Save submission\n",
    "    submission = pd.DataFrame({\n",
    "        \"Id\": test_ids,\n",
    "        \"SalePrice\": test_preds\n",
    "    })\n",
    "    submission.to_csv(f\"{name}_submission.csv\", index=False)\n",
    "    print(f\"{name} submission saved!\")\n",
    "\n",
    "    # Save model\n",
    "    joblib.dump(pipeline, f\"{name}_pipeline.pkl\")\n",
    "    print(f\"{name} pipeline saved!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe40c15c",
   "metadata": {},
   "outputs": [],
   "source": [
    "submission = pd.DataFrame({\n",
    "    \"Id\": test[\"Id\"],\n",
    "    \"SalePrice\": test_preds_exp\n",
    "})\n",
    "submission.to_csv(\"submission.csv\", index=False)\n",
    "print(\"submission.csv created!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "979c8585",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (venv)",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
